# LLM's

- OpenAI
  ToolChoice = ResponseToolChoice (none, auto, specified tool, etc) = ChatOptions ToolMode
  ParallelToolCallsEnabled
  FrequencyPenalty
  PresencePenalty

- Azure OpenAI - models
- Microsoft Foundry - models

# Workflows

- Previous Runs

# Conversations

- ChatMessage serializable (use AIJsonUtilities.DefaultOptions), would need to update our context serializing to notice and use this for serial/deserial
- Allow ModelCall to specify the path to get current conversation from (with create one if not present), then append call result to this same instance.

# Export and Import

- Workflows
- Metadata

# Minimum Viable Product Features

Max nodes executed for a workflow to prevent looping forever
Max number of runs to keep for a workflow to prevent too much data storage

# Integration Overrides

Max values for nodes, execution and runs to keep
	  
# Process

- GitHub account
  - Manual build (or tag) to create and publish a release
  - Docusaurus setup for generated static pages site (auto build and publish on changes)

- Build first version of documentation


# Implementation Details

Add UI editor as exposed via a specified path


# Futures

ChatKit output integration
User output, as there might be a stream of outputs
User input request, for LLM or any other part of a process

Models
	Support MCP Servers

Server
	Integrate use of OmniSharp for full intellisense





